{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Gas Storage and Temperature Outcomes\n",
    "\n",
    "This script pulls publicly available US temperature data and reported US natural gas storage activity, merges/formats that data using pandas DataFrames, and pushes the merged data to a SQL database\n",
    "\n",
    "Data is collected from the [Energy Information Administration's API](https://www.eia.gov/opendata/)\n",
    "and Climate Prediction Center's [FTP Site](ftp://ftp.cpc.ncep.noaa.gov/htdocs/degree_days/weighted/daily_data/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Import SQL Alchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Import and establish Base for which classes will be constructed \n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "Base = declarative_base()\n",
    "\n",
    "# Import modules to declare columns and column data types\n",
    "from sqlalchemy import Column, Integer, String, Float, Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Establish API key and organize a dictionary of API series id's for each storage region:**\n",
    "_The EIA API series IDs used in this analysis are listed [here](https://www.eia.gov/opendata/qb.php?category=1709237)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eia_api_key = '9e4c8d5761a387405ed003e062d45727'\n",
    "\n",
    "#this dictionary is a repository for the unique series ids for the Lower 48 and each EIA sub-region\n",
    "#... the regional keys established here are also referenced to establish dictionaries for the API calls and DataFrame conversions\n",
    "eia_api_series_ids = {\n",
    "    'l48' : 'NG.NW2_EPG0_SWO_R48_BCF.W',\n",
    "    'east' : 'NG.NW2_EPG0_SWO_R31_BCF.W',\n",
    "    'midwest' : 'NG.NW2_EPG0_SWO_R32_BCF.W',\n",
    "    'mountain' : 'NG.NW2_EPG0_SWO_R34_BCF.W',\n",
    "    'pacific' : 'NG.NW2_EPG0_SWO_R35_BCF.W',\n",
    "    'south_central' : 'NG.NW2_EPG0_SWO_R33_BCF.W',\n",
    "    'salt' : 'NG.NW2_EPG0_SSO_R33_BCF.W',\n",
    "    'nonsalt': 'NG.NW2_EPG0_SNO_R33_BCF.W'\n",
    "}\n",
    "\n",
    "#establishes base url for each API call\n",
    "#you just need to combine with the API ID string to complete each call\n",
    "base_url = f'http://api.eia.gov/series/?api_key={eia_api_key}&series_id='\n",
    "\n",
    "#establish empty dictionaries that will be used to save API calls and DataFrames\n",
    "call_dict = {}\n",
    "df_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create API calls:**\n",
    "\n",
    "_using a function that iterates through the regional names established in the keys for the eia_api_series_ids dictionary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_call(n):\n",
    "    return json.loads(requests.get(base_url + eia_api_series_ids[n]).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in eia_api_series_ids:\n",
    "    call_dict[n] = generate_call(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert JSON data from the API calls to pandas DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df(call):\n",
    "   return pd.DataFrame(call['series'][0]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in call_dict:\n",
    "    df_dict[n] = generate_df(call_dict[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format the regional DataFrames**\n",
    "1. _name columns_\n",
    "2. _convert 'week' column to datetime format_\n",
    "3. _set the index to 'week'_\n",
    "4. calculate week-over-week inventory change for each week\n",
    "5. _subtract 1 day from the weekly index date, to reflect the actual end date of each 'gas week'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(n):\n",
    "    df_dict[n].columns=['week',f'{n}_inventory']\n",
    "    df_dict[n]['week'] = pd.to_datetime(df_dict[n]['week'])\n",
    "    df_dict[n] = df_dict[n].sort_values('week')\n",
    "    df_dict[n] = df_dict[n].set_index('week')\n",
    "    df_dict[n][f'{n}_change'] = df_dict[n][f'{n}_inventory'] - df_dict[n][f'{n}_inventory'].shift(1)\n",
    "    df_dict[n].index = df_dict[n].index - datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in df_dict:\n",
    "    format_df(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eia_storage_data = df_dict['l48'].merge(\n",
    "    df_dict['east'], left_index = True, right_index = True).merge(\n",
    "    df_dict['midwest'], left_index = True, right_index = True).merge(\n",
    "    df_dict['mountain'], left_index = True, right_index = True).merge(\n",
    "    df_dict['pacific'], left_index = True, right_index = True).merge(\n",
    "    df_dict['south_central'], left_index = True, right_index = True).merge(\n",
    "    df_dict['salt'], left_index = True, right_index = True).merge(\n",
    "    df_dict['nonsalt'], left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Weather Data Pull**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pull HDDs**\n",
    "\n",
    "*add note on HDD definition*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = []\n",
    "for x in range(1981,datetime.datetime.now().year+1):\n",
    "    df = pd.read_csv(f'ftp://ftp.cpc.ncep.noaa.gov/htdocs/degree_days/weighted/daily_data/{x}/Population.Heating.txt',skiprows = 3, delimiter = '|').T\n",
    "    df.drop(df.index[0], inplace = True)\n",
    "    list_.append(df)\n",
    "CPC_HDDs = pd.concat(list_)\n",
    "CPC_HDDs.index = pd.to_datetime(CPC_HDDs.index)\n",
    "CPC_HDDs = CPC_HDDs[9]\n",
    "CPC_HDDs = pd.DataFrame(CPC_HDDs)\n",
    "CPC_HDDs.columns = ['HDDs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pull CDDs**\n",
    "\n",
    "*add note on CDD definition*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = []\n",
    "for x in range(1981,datetime.datetime.now().year+1):\n",
    "    df = pd.read_csv(f'ftp://ftp.cpc.ncep.noaa.gov/htdocs/degree_days/weighted/daily_data/{x}/Population.Cooling.txt',skiprows = 3, delimiter = '|').T\n",
    "    df.drop(df.index[0], inplace = True)\n",
    "    list_.append(df)\n",
    "CPC_CDDs = pd.concat(list_)\n",
    "CPC_CDDs.index = pd.to_datetime(CPC_CDDs.index)\n",
    "CPC_CDDs = CPC_CDDs[9]\n",
    "CPC_CDDs = pd.DataFrame(CPC_CDDs)\n",
    "CPC_CDDs.columns = ['CDDs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine degree day data into a single datagrame**\n",
    "\n",
    "...and add a Total Degree Day (TDDs) column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPC_TDDs = pd.merge(CPC_HDDs,CPC_CDDs, left_index = True, right_index = True)\n",
    "CPC_TDDs['TDDs'] = CPC_TDDs['HDDs'] + CPC_TDDs['CDDs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resample temperature data to report sum of degree days for each week ending Thursday**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPC_TDDs_Weekly = CPC_TDDs.resample('W-Thu').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, combine all temperature and storage data into a single DataFrame**\n",
    "\n",
    "... _and convert all columns to integers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(eia_storage_data,CPC_TDDs_Weekly, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in combined_df.columns:\n",
    "    combined_df[n] = pd.to_numeric(combined_df[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l48_inventory\n",
      "l48_change\n",
      "east_inventory\n",
      "east_change\n",
      "midwest_inventory\n",
      "midwest_change\n",
      "mountain_inventory\n",
      "mountain_change\n",
      "pacific_inventory\n",
      "pacific_change\n",
      "south_central_inventory\n",
      "south_central_change\n",
      "salt_inventory\n",
      "salt_change\n",
      "nonsalt_inventory\n",
      "nonsalt_change\n",
      "HDDs\n",
      "CDDs\n",
      "TDDs\n"
     ]
    }
   ],
   "source": [
    "for n in combined_df.columns:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create the SQL database**\n",
    "\n",
    "**Create the StorageWeather class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StorageWeather(Base):\n",
    "    __tablename__ = 'storage_weather'\n",
    "    week = Column(Date, primary_key = True)\n",
    "    l48_inventory = Column(Integer)\n",
    "    l48_change = Column(Integer)\n",
    "    east_inventory = Column(Integer)\n",
    "    east_change = Column(Integer)\n",
    "    midwest_inventory = Column(Integer)\n",
    "    midwest_change = Column(Integer)\n",
    "    mountain_inventory = Column(Integer)\n",
    "    mountain_change = Column(Integer)\n",
    "    pacific_inventory = Column(Integer)\n",
    "    pacific_change = Column(Integer)\n",
    "    south_central_inventory = Column(Integer)\n",
    "    south_central_change = Column(Integer)\n",
    "    salt_inventory = Column(Integer)\n",
    "    salt_change = Column(Integer)\n",
    "    nonsalt_inventory = Column(Integer)\n",
    "    nonsalt_change = Column(Integer)\n",
    "    HDDs = Column(Integer)\n",
    "    CDDs = Column(Integer)\n",
    "    TDDs = Column(Integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a connection to a local SQLite database**\n",
    "\n",
    "_note: will this by default create a new database if it doesn't already exist?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///storage.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the storage_weather table in the database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use a Session object to push the objects created and query the server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import Session\n",
    "session = Session(bind=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, save the DataFrame as a local .db file**\n",
    "\n",
    "_Question: how to I set the schema here?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_sql('storage_weather', schema = Base.metadata.create_all(engine), con = engine, index_label = 'week', if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, query the .db file using SQLAlchemy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlalchemy.orm.session.Session"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
